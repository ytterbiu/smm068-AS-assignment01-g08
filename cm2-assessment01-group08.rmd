---
title: "Computational Frameworks for Portfolio Construction and Risk Assessment"
subtitle: "SMM068 (2025-26) Group Coursework 01 - Group 08"
author:
  - "Benjamin Evans"
  - "Basmah K"
  - "Bong Su Kil"
  - "Ardi Wira Sudarmo"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  bookdown::html_document2:
    self_contained: true
    css: style.css
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: hide
    theme: united
    highlight: tango
    df_print: paged
  # runtime: shiny
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    pandoc_args:
      - "--lua-filter=answer.lua"
    number_sections: true
    toc: true
    latex_engine: xelatex
    citation_package: natbib
    keep_tex: true
# fontsize: 10pt
bibliography: references.bib
---

```{=latex}
\newpage
```

```{=html}
<h1>Information</h1>

This R Markdown document was created as part of a group assignment for SMM068 at Bayes Business School, City St George's, University of London in Term 2 2025-26.

```

```{r asthetic-header, include=FALSE, echo=FALSE}
# ==============================================================================
# SMM068 Financial Economics (Subject CM2)
# Group Coursework 2025-26
# Group:        Group 08
# Authors (in alphabetical order):
#   - Benjamin Evans
#   - Basmah Khan
#   - Bong Su Kil
#   - Ardi Wira Sudarmo
# Professor:    Dr. Iqbal Owadally
# Institution:  Bayes Business School - City St George's, University of London
# Date:         TBC~
# Description:  Term 2 group project for SMM068 Financial Economics
# (50% of coursework grade - 10% of module grade). The R code below
# has been exported directly from an R Markdown (.rmd) file.  Hence the knitr
# settings.
# Dependencies:
#   - TBI
# Acknowledgements
#   - TBI
# ==============================================================================

```

```{r setup-knitr, include=FALSE, echo=FALSE}
#----------------------- Initial setup (knitr settings) -----------------------#
dir.create("fig", showWarnings = FALSE)

# Defaults common to all outputs
knitr::opts_chunk$set(
  echo     = TRUE,
  message  = FALSE,
  warning  = FALSE,
  fig.align = "center",
  out.width = "100%",
  fig.path  = "fig/",
  dpi       = 300
)

# Output-specific settings
if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "pdf",
    fig.pos = "ht",
    out.extra = ""
  )
} else {
  knitr::opts_chunk$set(
    fig.width = 6,
    fig.height = 4,
    dev = "svglite"  # or "png"
  )
}
```

```{r setup-qol, include=FALSE, echo=FALSE}
#----------------------------- Clean environment ------------------------------#
rm(list = ls()) # Remove all objects
graphics.off() # Close all graphical devices
cat("\014") # Clean console
```

```{r load-dependencies, include=FALSE, echo=FALSE}
#------------------- Load dependencies / external libraries -------------------#
library(quantmod) # for downloading

library(kableExtra) # for general tables
library(DT) # for HTML interactive data tables
library(ggplot2) # for plotting
library(patchwork) # for combining multiple plots into one figure
library(plotly) # for interactive html plots

library(dplyr) # wrangle data for reporting
library(tidyr) # reshape data for reporting

# BE note: not sure if we will need xts or zoo, but have left in for now
library(xts) # for downloading / financial data analysis (used in CS1)
library(zoo) # for downloading / financial data analysis (used in CS1)

# BS note : for Q2
library(rootSolve) # This finds roots of (fairly straightforward) equations
library(optimx)   #  This handles non-linear unconstrained optimization (as well as box-constrained).
library(quadprog) # to solve a quadratic programming problem
library(scales) # to express portfolio weights as percentage
library(pander)


```

```{r html-app, echo=FALSE, results='asis', eval=knitr::is_html_output(), purl=FALSE}
#------------------------------ HTML link to app ------------------------------#
library(htmltools)

div(style = "background-color: #f8f9fa; padding: 20px; border: 1px solid #e9ecef; border-radius: 5px; text-align: center; margin-bottom: 30px;",
  h3("Bonus Interactive Dashboard Available"),
  p("This static report is accompanied by a live R Shiny dashboard allowing one to test different tickers, exclude specific outlier periods, and adjust bootstrap simulation parameters."),
  a(href = "https://3enji.shinyapps.io/SMM047-202526-Group07-Dashboard/",
    target = "_blank",
    class = "btn btn-primary",
    style = "background-color: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;",
    "Launch Interactive Dashboard")
)
```

```{r custom-functions, include=FALSE, echo=FALSE}
#---------------------------- Custom QOL functions ----------------------------#
#####################################
# function: banner comments (used to to section up code)
# Usage: banner_comment("Element 1: data cleaning") -> then ctrl + v (or cmd+v)
#####################################
banner_comment <- function(text, width = 80, border = "#", fill = "-") {
  txt <- paste0(" ", text, " ")
  inner_width <- width - 2 * nchar(border)
  banner_string <- ""

  if (inner_width <= nchar(txt)) {
    banner_string <- paste0(border, txt, border)
  } else {
    pad_total <- inner_width - nchar(txt)
    pad_left <- pad_total %/% 2
    pad_right <- pad_total - pad_left

    banner_string <- paste0(
      border,
      strrep(fill, pad_left),
      txt,
      strrep(fill, pad_right),
      border
    )
  }

  cat(banner_string, "\n")
  # copy banner to allow direct pasting (requires clipr)
  clipr::write_clip(banner_string)
  # avoid [1] when printing if want to manually copy
  invisible(banner_string)
}
#####################################
# function: format p-values for text
# Usage (in-line): `r format_p_vals(ad_test_result$p.value)`
# Usage (console): format_p_vals(ad_test_result$p.value)
#####################################
format_p_vals <- function(p) {
  if (length(p) != 1L || is.na(p)) {
    stop("Error! p must be a single non-missing value")
  }
  if (p > 1) {
    stop("Error! Value greater than 1")
  }
  if (p < 0) {
    stop("Error! Value less than 0")
  }

  if (p >= 0.01) {
    paste0("= ", formatC(p, format = "f", digits = 2))
  } else if (p >= 0.001) {
    paste0("= ", formatC(p, format = "f", digits = 3))
  } else {
    "< 0.001"
  }
}
#####################################
# function: format confidence intervals for tables & text
# Usage (in-line): `r format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2])`
# Usage (console): format_interval(el2_ci_normal_95[1], el2_ci_normal_95[2])
#####################################
format_interval <- function(lower, upper, digits = 3, small_interval = 5L) {
  paste0(
    "[",
    formatC(
      lower,
      format = "f",
      digits = digits,
      small.interval = small_interval,
      small.mark = " "
    ),
    ", ",
    formatC(
      upper,
      format = "f",
      digits = digits,
      small.interval = small_interval,
      small.mark = " "
    ),
    "]"
  )
}
#####################################
# function: format truncated ellipses
# Usage (in-line): `r tbi`
# Usage (console): tbi
#####################################
fmt_trunc_ellip <- function(
  x,
  digits = 7,
  ellip = "...",
  tol = 1e-12,
  trim_zeros_if_exact = TRUE
) {
  out <- rep(NA_character_, length(x))
  ok <- is.finite(x)
  scale <- 10^digits
  xt <- trunc(x[ok] * scale) / scale
  s <- formatC(xt, format = "f", digits = digits)
  add <- abs(x[ok] - xt) > tol * pmax(1, abs(x[ok]))
  if (trim_zeros_if_exact) {
    s_trim <- sub("0+$", "", s) # drop trailing zeros
    s_trim <- sub("\\.$", "", s_trim) # drop trailing decimal point
  } else {
    s_trim <- s
  }
  out[ok] <- ifelse(add, paste0(s, ellip), s_trim)
  out[is.na(x)] <- NA_character_
  out
}
```

# Downloading the data

```{r Q1, echo=knitr::is_html_output(), include=FALSE}
# ==============================================================================
# Q1 5 Stocks in U.S market
# ==============================================================================

# R code goes here
# Five insurance-related stocks are chosen
# The specific choice of stocks is flexible, as the primary objective

#stocks <- sort(c("TRV", "PGR", "ALL", "CB", "AIG"))
stocks <- sort(c("MSFT", "JNJ", "NEE", "AIG", "GS"))

start_date <- as.Date("2023-01-01")
end_date <- as.Date("2025-12-31")

cache_file <- paste0("cache_", paste(stocks, collapse = "_"), ".rds")

# Download only if file doesn't exist (avoiding throttling by Yahoo Finance).
# Without this code the stocks data are downloaded fresh every time wecompile
# the document (because we are using RMarkdown)
if (file.exists(cache_file)) {
  message("Loading from RDS cache...")
  prices <- readRDS(cache_file)
} else {
  message("Cache not found. Downloading from Yahoo Finance...")

  data_env <- new.env()
  getSymbols(
    stocks,
    src = "yahoo",
    from = start_date,
    to = end_date,
    env = data_env
  )

  # Get Adjusted Prices and merge dynamically (allows for more stocks)
  price_list <- lapply(data_env, Ad)
  prices_xts <- do.call(merge, price_list)

  prices <- data.frame(Date = index(prices_xts), coredata(prices_xts))
  # Ensure names match sorted list
  colnames(prices) <- c("Date", stocks)

  # Save to RDS (preserves datatypes)
  saveRDS(prices, cache_file)
}

# Q1-1 Adjusted close extract

head(prices)
tail(prices)

write.csv(prices, "Q1.csv", row.names = FALSE)
```

Download 3 years of daily prices for 5 U.S. stocks. [5 marks]

- pdf : Explain precisely which information resource you used, provide a URL if
  available, state the name and ticker code for the stocks, state the date
  range, state which price label you used.
- xlsx: The data should appear in worksheet “Q1” over 6 columns, the first
  column being the date.
- [Total: 5 marks (8.3%)]

**Answer:**

- **Information resource used:** Yahoo finance

- **URL if available:** https://finance.yahoo.com

- **Name and ticker code for stocks**
  - MSFT (Microsoft)
  - JNJ (Johnson & Johnson)
  - NEE (NextEra Energy, Inc)
  - AIG (American International Group, Inc)
  - GS (The Goldman Sachs Group, Inc.)


- **Date range:** 1 January 2023 – 31 December 2025

- **Price label used:** Adjusted closing price

```{r Q1-table-for-html, include=knitr::is_html_output(), echo=FALSE, message=FALSE, warning=FALSE}
if (knitr::is_html_output()) {
  datatable(
    prices,
    options = list(
      pageLength = 10,
      searching = TRUE,
      ordering = TRUE,
      autoWidth = TRUE,
      columnDefs = list(list(className = 'dt-center', targets = "_all"))
    ),
    caption = "Chosen insurance stock prices (Adjusted Close)",
    rownames = FALSE
  ) %>%
    # These are US stocks -> add USD currency format
    formatCurrency(columns = stocks, currency = "$") %>%
    formatStyle(
      'Date',
      target = 'row'
      # fontWeight = styleRow(1:10, "bold"),
    )
}
```

```{r Q1-stock-price-plot, eval = TRUE, echo = FALSE, fig.width=8, fig.height=5, fig.cap = "Adjusted daily closing prices for selected stocks (2023–2025).", fig.pos='!ht'}
# Transform to long format for ggplot
df_long <- prices %>%
  pivot_longer(
    cols = -Date,
    names_to = "Ticker",
    values_to = "Price"
  ) %>%
  mutate(Date = as.Date(Date))

# theme elements
base_theme <- theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    plot.margin = margin(10, 10, 10, 10)
  )

pal <- RColorBrewer::brewer.pal(8, "Dark2")
my_cols <- setNames(pal[c(1, 2, 3, 5, 4)], sort(unique(df_long$Ticker)))

# plot
p <- ggplot(df_long, aes(x = Date, y = Price, color = Ticker)) +
  geom_line(linewidth = 0.8, alpha = 0.9) +
  scale_color_manual(values = my_cols) +
  labs(
    title = "Performance of selected stocks over 3 years selected here...",
    subtitle = paste(
      "Adjusted closing prices:",
      min(df_long$Date),
      "to",
      max(df_long$Date)
    ),
    x = "Date",
    y = "Adjusted Close (USD)",
    color = "Stock:"
  ) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "6 months") +
  base_theme

if (knitr::is_html_output()) {
  # ggplotly(p)
  p_html <- p + geom_line(linewidth = 0.5, alpha = 0.9)
  ggplotly(p_html, height = 420) %>%
    layout(
      title = list(x = 0.5, xanchor = "center"),
      legend = list(
        orientation = "h",
        x = 0.5,
        xanchor = "center",
        y = -0.2,
        yanchor = "top"
      ),
      margin = list(t = 70, b = 90, l = 70, r = 20)
    ) %>%
    config(responsive = TRUE)
} else {
  p
}
```

```{=latex}
\newpage
```

# Portfolio optimisation

$$
\begin{array}{ll}
\hline
\textbf{Quantity} & \textbf{Expression} \\
\hline
\text{The portfolio return }  &
  R_p = \sum_{i=1}^{N} R_i
  \\[6pt]
\text{The expected portfolio return}
  &\overline{R}_p = \mathbb{E}[R_p] = \sum_{i=1}^{N} x_i \, \overline{R}_i \\[6pt]
\sigma_p \text{ is the standard deviation of return on the portfolio }
  & \sigma_p^2 = \sum_{i=1}^{N} x_i^2 \sigma_i^2 + 2 \sum_{i=1}^{N-1}
    \sum_{j=i+1}^{N} x_i x_j \rho_{ij} \sigma_i \sigma_j \\[6pt]
\text{Constraints 1}
  & \sum_{i=1}^{N} x_i = 1  \\[6pt]
\text{Constraints 2}
  & \sum_{i=1}^{N}x_i\overline{R_i} = E \\[6pt]
\hline
\end{array}
$$

```{r Q2-working, echo=FALSE}
# ==============================================================================
# Q2(a) estimate all required statistics for the 5 stocks
# ==============================================================================

# prices_mat <- as.matrix(prices[, -1])
# returns <- log(prices_mat[-1, ]) - log(prices_mat[-nrow(prices_mat), ])
#
# colnames(returns) <- colnames(prices_mat)
#
# mean_returns_d <- colMeans(returns)
# cov_matrix_d <- cov(returns)
# sd_d  <- sqrt(diag(cov_matrix_d))
#
# # Q2(a)_ annual expectation and covariance
#

# drop date column
px <- prices[, -1]

#%returns
data1 <- lapply(px, function(x) {
  ROC(as.numeric(x), type = "discrete") * 100  
})

ret1 <- as.data.frame(do.call(cbind, data1))
colnames(ret1) <- colnames(px)


# remove the first row of missing values
ret1 <- ret1[-1, , drop = FALSE]

# add dates column
ret1 <- data.frame(Date = prices$Date[-1], ret1)

# plot the data
pander(head(ret1), split.table = Inf)

# convert to long

ret_long = pivot_longer(ret1, cols = -c(Date), values_to = "Return", names_to = "Stock")

# plot

port_p1 = ggplot(ret_long, aes(Date, Return, color = Stock)) + geom_path(stat = "identity") +
    facet_grid(Stock ~ .) + theme_minimal() + labs(x = "Date", y = "Returns")
port_p1

R_pct <- as.matrix(ret1[, -1])

mean_returns_d <- colMeans(R_pct)
cov_matrix_d   <- cov(R_pct)
sd_d           <- sqrt(diag(cov_matrix_d))

# BE note to check: why factors of 3/100?
# /3 to get 1 year's worth of data
# then *100 to get 1 year as a %
mean_returns_a <- mean_returns_d * nrow(R_pct) /3/100
cov_matrix_a <- cov_matrix_d* nrow(R_pct) / 3/100
sd_a <- sd_d*sqrt(cov_matrix_d* nrow(R_pct) / 3/100)
N_return <- length(mean_returns_a)

```

## Q2(a)

Daily returns were calculated from the adjusted closing prices of the five stocks. The expected returns were obtained by taking the average of the daily returns and the annualised.

<!-- BE note: !look to put below in loop that auto-fetches stock name -->
<!-- prettier-ignore -->
::: {.answer-wrapper}
::: {.answer}
Answer:

`r sprintf("%s %.3f",colnames(ret1)[2], mean_returns_a[1]*100)`% (3dp) 
`r sprintf("%s %.3f",colnames(ret1)[3], mean_returns_a[2]*100)`% (3dp) 
`r sprintf("%s %.3f",colnames(ret1)[4], mean_returns_a[3]*100)`% (3dp) 
`r sprintf("%s %.3f",colnames(ret1)[5], mean_returns_a[4]*100)`% (3dp) 
`r sprintf("%s %.3f",colnames(ret1)[6], mean_returns_a[5]*100)`% (3dp)

:::
:::


<!-- # ```{r, echo=FALSE} -->
<!-- # df <- data.frame( -->
<!-- #   Stock = stocks, -->
<!-- #   # Format: "12.345% (3dp)" -->
<!-- #   Return = sprintf("%.3f (3dp)", mean_returns_a * 100)  -->
<!-- # ) -->
<!-- #  -->
<!-- # kable(df,  -->
<!-- #       col.names = c("Stock", "Mean Return perc. (3dp)"),  -->
<!-- #       caption = "Answer: Portfolio Returns", -->
<!-- #       booktabs = TRUE, -->
<!-- #       escape = FALSE, -->
<!-- #       align = "lc", -->
<!-- # ) %>% -->
<!-- #   kable_styling( -->
<!-- #     full_width = FALSE, -->
<!-- #     position = "center", -->
<!-- #     latex_options = "hold_position" -->
<!-- #   ) -->
<!-- # ``` -->

The variance–covariance matrix were also estimated using the same daily returns and annualised accordingly.
The results show that the stocks have different levels of return and risk

```{r Q2a, echo=FALSE}
cov_matrix_a
```

```{=latex}
\newpage
```

## Q2b

```{r Q2b-working, echo=FALSE}
# ==============================================================================
# Q2(b) estimate all required statistics for the 5 stocks
# ==============================================================================
# method log return
# ==============================================================================

# number of securities
n_sec <- ncol(cov_matrix_a)

# Budget constraint
ct_1_lhs <- rep(1, n_sec)
ct_1_rhs <- 1

#  Target return constraint: x_1 mu_1 + x_2 mu_2 + x_3 mu_3 + x_4 mu_4
#                            + x_5 mu_5= mu_target <=> mu^T x = mu_target

ct_2_lhs <- mean_returns_a
ct_2_rhs <- mean(mean_returns_a)

#  Optimization. There is one constraint: the budget constraint

dmat <- cov_matrix_a

# dvec is a column vector of 0's
dvec <- cbind(rep(0, n_sec))
amat <- rbind(ct_1_lhs, diag(n_sec)) # Creates a row vector
bvec <- c(ct_1_rhs, rep(0, n_sec))
meq <- 1 # One equality constraint

# Solve for global min var portfolio. t() is the transpose function
pf_gbl <- solve.QP(dmat, dvec, t(amat), bvec, meq)
pf_gbl$solution

# Optimization. There are two constraints:
# budget constraint and target expected return constraint
amat <- rbind(ct_1_lhs, ct_2_lhs, diag(n_sec))
bvec <- c(ct_1_rhs, ct_2_rhs, rep(0, n_sec))
meq <- 2 # Two equality constraints
# Solve for required portfolio. t() is the transpose function
pf_mvf <- solve.QP(dmat, dvec, t(amat), bvec, meq)
# State the required portfolio
pf_mvf$solution

# target
targets <- seq(min(mean_returns_a), max(mean_returns_a), length.out = 200)

# BE note: want to understand this function
res <- lapply(targets, function(tar) {
  amat <- rbind(ct_1_lhs, ct_2_lhs, diag(N_return))
  bvec <- c(ct_1_rhs, tar, rep(0, N_return))

  sol <- tryCatch(
    solve.QP(dmat, dvec, t(amat), bvec, meq = 2),
    error = function(e) NULL
  )
  if (is.null(sol)) {
    return(NULL)
  }

  w <- sol$solution
  w[w < 1e-10] <- 0
  w <- w / sum(w)

  c(
    risk = sqrt(t(w) %*% cov_matrix_a %*% w),
    ret = sum(w * mean_returns_a)
  )
})

res <- do.call(rbind, res)

# clean weights
w_opt <- pf_mvf$solution
w_opt[w_opt < 1e-10] <- 0
w_opt <- w_opt / sum(w_opt)

# optimal portfolio coordinates
opt_mean <- sum(w_opt * mean_returns_a)
opt_sd <- sqrt(drop(t(w_opt) %*% cov_matrix_a %*% w_opt))

# plot
plot(
  sqrt(diag(cov_matrix_a)),
  mean_returns_a,
  pch = 1,
  cex = 1.2,
  xlab = "StdDev",
  ylab = "mean",
  main = "Mean Variance Portfolio",
  xlim = range(sqrt(diag(cov_matrix_a))) * c(0.9, 1.1),
  ylim = range(mean_returns_a) * c(0.9, 1.1)
)

# labels for assets
text(
  sqrt(diag(cov_matrix_a)),
  mean_returns_a,
  labels = names(mean_returns_a),
  pos = 4,
  cex = 0.9
)

# optimal point (blue)
points(opt_sd, opt_mean, pch = 16, col = "blue", cex = 1.3)
text(opt_sd, opt_mean, labels = "Optimal", pos = 4, col = "blue")
grid()

plot(
  res[, "risk"],
  res[, "ret"],
  type = "l",
  col = "red",
  lwd = 1,
  xlab = "Mean-Var target Risk",
  ylab = "Expected Return",
  xlim = range(res[, "risk"]) * c(0.98, 1.02),
  main = "Efficient Frontier (No short selling)"
)
grid()
i_gmv <- which.min(res[, "risk"])

lines(
  res[i_gmv:nrow(res), "risk"],
  res[i_gmv:nrow(res), "ret"],
  col = "darkgreen",
  lwd = 2
)
asset_cols <- c("red", "blue", "orange", "purple", "brown")
points(
  sqrt(diag(cov_matrix_a)),
  mean_returns_a,
  pch = 16,
  cex = 1.3,
  col = asset_cols
)

text(
  sqrt(diag(cov_matrix_a)),
  mean_returns_a,
  labels = names(mean_returns_a),
  pos = 4,
  cex = 0.9,
  col = asset_cols
)

```

## Q2c



# Asset risk theory {#qthree}

```{r Q3-working, echo=FALSE}
r1_mu <- 0.06
r1_sd <- sqrt(0.0244)

r2_p1 <- 0.8
r2_p2 <- 0.2
r2_mu1 <- 0.0
r2_mu2 <- 0.3
r2_sd1 <- 0.1
r2_sd2 <- 0.1

# Derived values for the working steps
r2_mu_bar <- (r2_p1 * r2_mu1) + (r2_p2 * r2_mu2)

q3a_ev_step1 <- r2_p1 * (r2_sd1^2)
q3a_ev_step2 <- r2_p2 * (r2_sd2^2)
q3a_ev_total <- q3a_ev_step1 + q3a_ev_step2

q3a_ve_step1 <- r2_p1 * (r2_mu1 - r2_mu_bar)^2
q3a_ve_step2 <- r2_p2 * (r2_mu2 - r2_mu_bar)^2
q3a_ve_total <- q3a_ve_step1 + q3a_ve_step2

r2_var <- q3a_ev_total + q3a_ve_total
```

Consider 2 assets with random returns. Asset 1 has a Normally distributed return
$R_{1}$ with mean 6% and variance 2.44% and asset 2 has a random return $R_{2}$
(where w.p. means "with probability").

$$
R_{1} \sim \mathcal{N}(\mu=0.06, \sigma^{2}=0.0244),
\quad
R_{2}
\sim
\begin{cases}
\mathcal{N}(\mu=0.0, \, \sigma^{2}=0.1^{2}) \quad &\text{w.p. } 0.8\\
\mathcal{N}(\mu=0.3, \, \sigma^{2}=0.1^{2}) &\text{w.p. } 0.2
\end{cases}
$$

## Calculate $\mathbb{E}[R_{2}]$ and $\mathrm{Var}(R_{2})$. _[5 marks]_ {#qthreePtA}

$$
\begin{array}{ll}
\hline
\textbf{Quantity} & \textbf{Expression} \\
\hline
\text{Distribution of }R_{2}  &
  \begin{cases}
  X\mid Z =1 &\sim \mathcal{N}(\mu=0.0, \, \sigma^{2}=0.1^{2}) \quad \text{w.p. } 0.8\\
  X\mid Z =2 &\sim \mathcal{N}(\mu=0.3, \, \sigma^{2}=0.1^{2}) \quad \text{w.p. } 0.2\\[2pt]
  \end{cases} \\[8pt]
  \hline
\text{Expectation}
  & \mathbb{E}[R_{2}] = \sum_{z=1}^2 \mathbb{P}(Z=z) \times \mu_z \\[6pt]
\text{Law of total variance}
  & \text{Var}(R_{2}) = \mathbb{E}\big[\text{Var}(X\mid Z)\big] +
  \text{Var}\big(\mathbb{E}[X\mid Z]\big) \\[6pt]
\text{Expectation of variance}
  & \mathbb{E}\big[\text{Var}(X\mid Z)\big] =
  \sum_{z=1}^2 \mathbb{P}(Z=z) \times \sigma_z^2  \\[6pt]
\text{Variance of expectation}
  & \text{Var}\big(\mathbb{E}[X\mid Z]\big) =
  \sum_{z=1}^2 \mathbb{P}(Z=z) \times \left(\mu_z - \mathbb{E}[R_{2}]\right)^2 \\[6pt]
\hline
\end{array}
$$

<!-- prettier-ignore -->
\begin{align*} 
\mathbb{E}[R_{2}] & = (`r r2_mu1` \times `r r2_p1`) + (`r r2_mu2` \times `r r2_p2`) = `r fmt_trunc_ellip(r2_mu_bar)` \\ 
\mathbb{E}[\text{Var}(X\mid Z)] 
  & = p_{1} \sigma_{1}^{2} + p_{2} \sigma_{2}^{2} \\ 
  & = (`r r2_p1`)(`r r2_sd1`^2) + (`r r2_p2`)(`r r2_sd2`^2) = `r q3a_ev_total` \\ 
\text{Var}(\mathbb{E}[X\mid Z]) 
  & = (`r r2_p1` \times (`r r2_mu1` - `r r2_mu_bar`)^2) + (`r r2_p2` \times (`r r2_mu2` - `r r2_mu_bar`)^2) \\
  & = `r fmt_trunc_ellip(q3a_ve_step1)` + `r fmt_trunc_ellip(q3a_ve_step2)` = `r fmt_trunc_ellip(q3a_ve_total)` \\
\text{Var}(R_{2}) & = `r q3a_ev_total` + `r q3a_ve_total` = `r r2_var`
\end{align*}

<!-- prettier-ignore -->
::: {.answer-wrapper} 
::: {.answer} 
Answers: $\mathbb{E}[R_{2}] =$ `r fmt_trunc_ellip(r2_mu_bar)`, $\text{Var}(R_{2})=$ `r fmt_trunc_ellip(r2_var)`
::: 
:::

Point to note: The first two moments about the mean of asset one and asset two
(the mean and the variance) are the same.

## Use **R** to estimate $\mathbb{E}[R_{2}]$ and $\mathrm{Var}(R_{2})$. _[5 marks]_ {#qthreePtB}

```{r Q3b, echo=TRUE, results = 'hide', fig.cap="Code"}
# Set seed for reproducibility
set.seed(1234)
# Set the number of simulations
n_sim <- 1e6

# Z can be considered to be ~Binomial(n=n_sim, P=weights for each part)
# R is then simulated as normal with mean that depends upon value of Z
Z <- rbinom(n_sim, size = 1, prob = 0.2)
R2 <- rnorm(n_sim, mean = ifelse(Z==1, 0.3, 0.0), sd = 0.1)

# Calculate mean and variance
R_2_mean <- mean(R2)
R_2_var <- var(R2)
```

Monte Carlo simulation was used to estimate $\mathbb{E}[R_{2}]$ and $\mathrm{Var}(R_{2})$ using **R**. An indicator $Z \sim \text{Bernoulli}(0.2)$ was used to choose the mixture component, and values for $R_{2}\mid Z$ were generated from a normal distribution with standard deviation $0.1$ and mean $0$ (if $Z=0$) or $0.3$ (if $Z=1$).

The sample mean and variance of the simulated values provide estimates for $\mathbb{E}[R_{2}]$ and $\mathrm{Var}(R_{2})$ ($\widehat{\mathbb{E}[R_{2}]}$ and $\widehat{\mathrm{Var}(R_{2})}$ respectively).

Using `r formatC(n_sim, format="fg", digits=3, big.mark=",")` simulations, one obtains
$\widehat{\mathbb{E}[R_{2}]}=$ `r fmt_trunc_ellip(R_2_mean)`
and
$\widehat{\mathrm{Var}(R_{2})}=$ `r fmt_trunc_ellip(R_2_var)`, or
`r formatC(R_2_mean, format="fg", digits=5, small.mark=" ", small.interval=3L)`
and
`r formatC(R_2_var, format="fg", digits=5, small.mark=" ", small.interval=3L)` to 5 significant figures.

These are very similar/in agreement, as expected, to the theoretical values of
`r fmt_trunc_ellip(r2_mu_bar)` and `r fmt_trunc_ellip(r2_var)`
calculated in \@ref(qthreePtA).

<!-- `r fmt_trunc_ellip(mean(R2))` -->

_Notes on the R functions: `rbinom(n, size=1, prob=p)` generates $n$ Bernoulli($p$) draws (0/1 indicators), and `rnorm(n, mean, sd)` generates $n$ normal draws; here mean is a vector so the mean changes with $Z$._

- =BE note: Do we want a table of results here?, i.e., something like this:=

| Quantity              | Simulation                    | Theoretical                    |
| :-------------------- | :---------------------------- | :----------------------------- |
| $\mathbb{E}[R_{2}]$   | `r fmt_trunc_ellip(R_2_mean)` | `r fmt_trunc_ellip(r2_mu_bar)` |
| $\mathrm{Var}(R_{2})$ | `r fmt_trunc_ellip(R_2_var)`  | `r fmt_trunc_ellip(r2_var)`    |

```{r Q3b-convergence-table-ci, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1234)
p <- 0.2
mu0 <- 0.0
mu1 <- 0.3
sd <- 0.1

# simulation sizes: 10^2 ... 10^7
checkpoints <- 10^(2:7)

# streaming mean/variance (Welford)
# https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
chunk_size <- 1e6
n <- 0
mean_run <- 0
M2 <- 0

update_running <- function(x, n, mean_run, M2) {
  m <- length(x)
  xbar <- mean(x)
  xM2 <- sum((x - xbar)^2)
  n_new <- n + m
  delta <- xbar - mean_run
  mean_new <- mean_run + delta * m / n_new
  M2_new <- M2 + xM2 + delta^2 * n * m / n_new
  list(n = n_new, mean = mean_new, M2 = M2_new)
}

out <- vector("list", length(checkpoints))

for (i in seq_along(checkpoints)) {
  target <- checkpoints[i]
  remaining <- target - n

  while (remaining > 0) {
    m <- min(chunk_size, remaining)
    Z <- rbinom(m, size = 1, prob = p)
    x <- rnorm(m, mean = ifelse(Z == 1, mu1, mu0), sd = sd)

    upd <- update_running(x, n, mean_run, M2)
    n <- upd$n
    mean_run <- upd$mean
    M2 <- upd$M2

    remaining <- target - n
  }

  var_run <- M2 / (n - 1)
  out[[i]] <- data.frame(n_sim = n, mean = mean_run, var = var_run)
}

z <- qnorm(0.975)
ci_digits <- 5

sim_df <- bind_rows(out) %>%
  mutate(
    n_label = paste0("$10^{", as.integer(log10(n_sim)), "}$"),

    # 95% CI for mean (CLT)
    se_mean = sqrt(var / n_sim),
    mean_lo = mean - z * se_mean,
    mean_hi = mean + z * se_mean,

    # 95% CI for variance (chi-square; exact if normal, approx for mixture)
    df = n_sim - 1,
    var_lo = df * var / qchisq(0.975, df),
    var_hi = df * var / qchisq(0.025, df),

    mean_5sf = formatC(
      mean,
      format = "fg",
      digits = 4,
      flag = "#"
      # small.interval = 3L,
      # small.mark = " "
    ),
    var_5sf = formatC(
      var,
      format = "fg",
      digits = 4,
      flag = "#"
      # small.interval = 3L,
      # small.mark = " "
    ),
    mean_ci = format_interval(mean_lo, mean_hi, digits = ci_digits),
    var_ci = format_interval(var_lo, var_hi, digits = ci_digits)
  ) %>%
  select(
    `Simulations` = n_label,
    `Mean` = mean_5sf,
    `95 percent CI (mean)` = mean_ci,
    `Variance` = var_5sf,
    `95 percent CI (var)` = var_ci
  )

sim_df %>%
  kable(
    booktabs = TRUE,
    escape = FALSE,
    align = "lcccc",
    linesep = c("", "", "\\addlinespace"),
    caption = "Monte Carlo convergence for $\\mathbb{E}[R_2]$ and $\\mathrm{Var}(R_2)$ with 95 percent intervals."
  ) %>%
  kable_styling(
    full_width = FALSE,
    position = "center",
    latex_options = "hold_position"
  )
```

<!-- # ================================================================== -->
<!-- # Q3(C)  Calculate the shortfall probabilities for asset 1
<!-- # ================================================================== -->

## Calculate the shortfall probabilities for asset 1, assuming a benchmark return of _[2 marks]_ {#qthreePtC}

$$
\begin{array}{ll}
\hline
\textbf{Quantity} & \textbf{Expression} \\
\hline
\text{Distribution of } R_1
& R_1 \sim \mathcal{N}(\mu = 0.06,\ \sigma^2 = 0.0244) \\[6pt]
\text{Expected return}
& \mathbb{E}[R_1] = 0.06 \\[6pt]
\text{Standard deviation}
& \sigma = \sqrt{0.0244} \approx 0.156205 \\[6pt]
\text{Shortfall probability}
& \mathbb{P}[R_1 < L]
= \int_{-\infty}^{L} f_{R_1}(x)\, \mathrm{d}x \\[6pt]
\hline
\end{array}
$$

```{r Q3c, echo=FALSE}
thresholds <- c(T1 = 0, T2 = -0.1)
w <- c(r2_p1, r2_p2)
means <- c(r2_mu1, r2_mu2)
sds <- c(r2_sd1, r2_sd2)

# calc shortfall prob for R2
calc_shortfall_r2 <- function(L) {
  probs <- pnorm(L, mean = means, sd = sds)
  sum(w * probs)
}

# apply to both thresholds
results_r1 <- pnorm(thresholds, mean = r1_mu, sd = r1_sd)
results_r2 <- sapply(thresholds, calc_shortfall_r2)
```

<!-- # ================================================================== -->
<!-- # Q3(C)(i)  benchmark 0%
<!-- # ================================================================== -->

### benchmark = 0%,

<!-- prettier-ignore -->
\begin{align*} 
\mathbb{P}[R_1 < 0] 
&= \int_{-\infty}^{0} f_R(x)\mathrm{d}x \\
&= \Phi\left( \frac{0 - 0.06}{0.156205} \right) \\
&= \Phi\left(`r round((0-0.06)/0.156205,4)`\right) \\
&= `r fmt_trunc_ellip(results_r1["T1"])` 
\end{align*}

<!-- prettier-ignore -->
::: {.answer-wrapper}
::: {.answer}
Answer: `r sprintf("%.3f", results_r1["T1"]*100)`% (3dp)
:::
:::

<!-- # ================================================================== -->
<!-- # Q3(C)(ii)  benchmark -10%
<!-- # ================================================================== -->

### benchmark = -10%.

<!-- prettier-ignore -->
\begin{align*} 
  \mathbb{P}[R_1 < -0.1] 
  &= \int_{-\infty}^{-0.1} f_R(x)\mathrm{d}x \\
  &= \Phi \left( \frac{-0.1 - 0.06}{0.156205} \right) \\
  &= \Phi \left(`r round((-0.1-0.06)/0.156205,4)`\right)\\
  &=`r fmt_trunc_ellip(results_r1["T2"])`
\end{align*}

<!-- prettier-ignore -->
::: {.answer-wrapper}
::: {.answer} 
Answer: `r sprintf("%.3f", results_r1["T2"]*100)`% (3dp)
:::
:::

<!-- # ================================================================== -->
<!-- # Q3(d)  Calculate the shortfall probabilities for asset 2
<!-- # ================================================================== -->

## Repeat part Q3c above for asset 2. {#qthreePtD}

_[4 marks]_

$$
\begin{array}{ll}
\hline
\textbf{Quantity} & \textbf{Expression} \\
\hline
\text{Distribution of }R_{2}  & \begin{cases}
X\mid Z =1 &\sim \mathcal{N}(\mu=0.0, \, \sigma^{2}=0.1^{2}) \quad \text{w.p. } 0.8\\
X\mid Z =2 &\sim \mathcal{N}(\mu=0.3, \, \sigma^{2}=0.1^{2}) \quad \text{w.p. } 0.2
\end{cases} \\[8pt]
\text{Shortfall probability}
& \mathbb{P}[R_2 < L]
= 0.8 \int_{-\infty}^{L} f_{\mathcal{N}(0,\,0.1^2)}(x)\, \mathrm{d}x
+ 0.2 \int_{-\infty}^{L} f_{\mathcal{N}(0.3,\,0.1^2)}(x)\, \mathrm{d}x \\[6pt]
\hline
\end{array}
$$

<!-- # ================================================================== -->
<!-- # Q3(d)(i)  benchmark 0%
<!-- # ================================================================== -->

### benchmark = 0%,

<!-- prettier-ignore -->
\begin{align*} 
  \mathbb{P}[R_2 < 0] 
  &= 0.8 \int_{-\infty}^{0} f_{\mathcal{N}(0,\,0.1^2)}(x)\, \mathrm{d}x + 
    0.2 \int_{-\infty}^{0} f_{\mathcal{N}(0.3,\,0.1^2)}(x)\, \mathrm{d}x \\
  &= 0.8 \times \Phi \left( \frac{0 - 0}{0.1} \right) + 
    0.2 \times \Phi \left( \frac{0 - 0.3}{0.1} \right) \\
    &= 0.8 \times \Phi \left(0\right) + 0.2 \times \Phi \left(-3\right) \\
    &=`r fmt_trunc_ellip(results_r2["T1"])`
\end{align*}

<!-- prettier-ignore -->
::: {.answer-wrapper}
::: {.answer} 
Answer: `r sprintf("%.3f", results_r2["T1"]*100)`% (3dp)
:::
:::

<!-- # ================================================================== -->
<!-- # Q3(d)(ii)  benchmark -10%
<!-- # ================================================================== -->

### benchmark = -10%,

benchmark =-10%

<!-- prettier-ignore -->
\begin{align*} 
  \mathbb{P}[R_2 < -0.1] 
  &= 0.8 \int_{-\infty}^{-0.1} f_{\mathcal{N}(0,\,0.1^2)}(x)\, \mathrm{d}x + 
  0.2 \int_{-\infty}^{-0.1} f_{\mathcal{N}(0.3,\,0.1^2)}(x)\, \mathrm{d}x \\
  &= 0.8 \times \Phi \left( \frac{-0.1 - 0}{0.1} \right) + 0.2 \times 
    \Phi \left( \frac{-0.1 - 0.3}{0.1} \right) \\
    &= 0.8 \times \Phi \left(-1\right) + 0.2 \times \Phi \left(-4\right) \\
    &=`r fmt_trunc_ellip(results_r2["T2"])`
\end{align*}

<!-- prettier-ignore -->
::: {.answer-wrapper}
::: {.answer} 
Answer: `r sprintf("%.3f", results_r2["T2"]*100)`% (3dp)
:::
:::

<!-- # ================================================================== -->
<!-- # Q3(e)  Plot pdfs of returns for assets 1 and 2 on the same axes
<!-- # ================================================================== -->

## Use **R** to plot the probability density functions of returns for assets 1 and 2 on the same axes. _[3 marks]_ {#qthreePtE}

The plotting range $[-0.5, 0.6]$ was selected based on definite outlier. For
Asset1, $R_1 \sim \mathcal{N}(0.06, 0.0244)$ with standard deviation
$\sigma_1 = \sqrt{0.0244} \approx 0.156205$, giving a three-sigma inverval of
$0.06 \pm 3\times\sigma_1 \approx [-0.41, 0.53]$. For asset 2, the two normal
components have means 0 and 0.3 with standard deviation 0.1, resulting in
three-sigma intervals of $[-0.3, 0.3]$ and $[0.0, 0.6]$ , respectively. The
selected range $[-0.5, 0.6]$ therefore covers approximately three standard
deviations for both assets.

```{r Q3-part-e-v1, echo=FALSE}
x <- seq(-0.5, 0.6, by = 0.001)
R1_pdf <- dnorm(x, mean = 0.06, sd = sqrt(0.0244))
R2_pdf <- 0.8 * dnorm(x, mean = 0, sd = 0.1) + 0.2 * dnorm(x, mean = 0.3, sd = 0.1)

ymax <- max(R1_pdf, R2_pdf)
plot(
  x,
  R1_pdf,
  type = "l",
  col = "red",
  lwd = 2,
  ylim = c(0, ymax),
  xlab = "Return",
  ylab = "Density",
  main = "Probability density functions of returns for assets 1 and 2"
)
grid()
lines(x, R2_pdf, type="l", lwd =2, col = "blue")

legend(
  "topright",
  legend = c("R1_PDF", "R2_PDF"),
  col = c("red", "blue"),
  lty = c(1, 1),
  lwd = 2,
  bty = "n"
)

plot(
  x,
  R2_pdf - R1_pdf,
  type = "l",
  lwd = 2,
  col="green",
  xlab = "Return",
  ylab = "PDF difference (R2 - R1)",
  main = "Difference between PDFs"
)
abline(h = 0, lty = 2)
grid()

```

```{r Q3-part-e-ggplot, eval = TRUE, echo = FALSE, fig.width=6, fig.height=6.0, fig.cap = "Comparison of Asset 1 and Asset 2 return distributions. Top shows probability density functions for assets 1 and 2 and the bottom plot shows the point-wise difference in density (Asset 2 minus Asset 1), highlighting the regions of relative over- and under-exposure.", fig.pos='!ht'}

x_vals <- seq(-0.6, 0.6, by = 0.001)

threshold_df <- data.frame(
  x_int = thresholds,
  name = names(thresholds)
)

pdf_r1 <- dnorm(x_vals, mean = r1_mu, sd = r1_sd)
pdf_r2 <- r2_p1 *
  dnorm(x_vals, mean = r2_mu1, sd = r2_sd1) +
  r2_p2 * dnorm(x_vals, mean = r2_mu2, sd = r2_sd2)

plot_df <- tibble(
  x = x_vals,
  R1 = pdf_r1,
  R2 = pdf_r2,
  diff = pdf_r2 - pdf_r1
)

base_theme <- theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
    plot.margin = margin(5.5, 5.5, 5.5, 5.5),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    legend.position = "top",
    legend.title = element_blank()
  )

df_long <- plot_df %>%
  select(x, R1, R2) %>%
  pivot_longer(cols = c(R1, R2), names_to = "Asset", values_to = "Density")

p1_pdf <- ggplot(df_long, aes(x = x, y = Density, color = Asset)) +
  geom_line(linewidth = 0.8) +
  geom_vline(
    data = threshold_df,
    aes(xintercept = x_int),
    linetype = "dotted",
    color = "black",
    linewidth = 0.6
  ) +
  geom_text(
    data = threshold_df,
    aes(x = x_int, y = 0.1, label = name),
    inherit.aes = FALSE,
    angle = 0,
    vjust = -0.5,
    size = 3.5,
    fontface = "bold"
  ) +
  scale_color_manual(values = c("R1" = "red", "R2" = "blue")) +
  labs(
    title = "Probability Density Functions of Returns",
    x = NULL,
    y = "Density"
  ) +
  base_theme

p2_pdf <- ggplot(plot_df, aes(x = x, y = diff)) +
  geom_line(color = "darkgreen", linewidth = 0.8) +
  geom_vline(
    data = threshold_df,
    aes(xintercept = x_int),
    linetype = "dotted",
    color = "black",
    linewidth = 0.6
  ) +
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    alpha = 0.5
  ) +
  labs(
    title = "Difference between PDFs (Asset 2 - Asset 1)",
    x = "Return",
    y = "Density"
  ) +
  base_theme

(p1_pdf / p2_pdf) + plot_layout(heights = c(1.2, 1))
```

```{=latex}
\newpage
```

<!-- # ================================================================== -->
<!-- # Q3(f) plot the cdfs of returns for assets 1 and 2 on the same axes.
<!-- # ================================================================== -->

## Use **R** to plot the cumulative density functions of returns for assets 1 and 2 on the same axes. _[2 marks]_ {#qthreePtF}

```{r Q3-part-f-v1, echo=FALSE}

R1_cdf <- pnorm(x, mean = 0.06, sd = sqrt(0.0244))
R2_cdf <- 0.8 * pnorm(x, mean = 0, sd = 0.1) + 0.2 * pnorm(x, mean = 0.3, sd = 0.1)

ymax_cdf <- max(R1_cdf, R2_cdf)
plot(
  x,
  R1_cdf,
  type = "l",
  col = "red",
  lwd = 2,
  ylim = c(0, ymax_cdf),
  xlab = "Return",
  ylab = "CDF",
  main = "Cumulative density functions of returns for assets 1 and 2"
)
grid()
lines(x, R2_cdf, type="l", lwd =2, col = "blue")

legend(
  "topright",
  legend = c("R1_CDF", "R2_CDF"),
  col = c("red", "blue"),
  lty = c(1, 1),
  lwd = 2,
  bty = "n"
)

plot(
  x,
  R2_cdf - R1_cdf,
  type = "l",
  lwd = 2,
  col="green",
  xlab = "Return",
  ylab = "CDF difference (R2 - R1)",
  main = "Difference between CDFs"
)
abline(h = 0, lty = 2)
grid()

```

```{r Q3-part-f-ggplot, eval = TRUE, echo = FALSE, fig.width=6, fig.height=6.0, fig.cap = "Comparison of asset 1 and asset 2 cumulative distribution functions. Threshold values used are marked on both graphs...", fig.pos='!ht'}

cdf_r1 <- pnorm(x_vals, mean = r1_mu, sd = r1_sd)
cdf_r2 <- r2_p1 *
  pnorm(x_vals, mean = r2_mu1, sd = r2_sd1) +
  r2_p2 * pnorm(x_vals, mean = r2_mu2, sd = r2_sd2)

cdf_df <- tibble(
  x = x_vals,
  R1 = cdf_r1,
  R2 = cdf_r2,
  diff = cdf_r2 - cdf_r1
)

cdf_long <- cdf_df %>%
  select(x, R1, R2) %>%
  pivot_longer(cols = c(R1, R2), names_to = "Asset", values_to = "Probability")

p1_cdf <- ggplot(cdf_long, aes(x = x, y = Probability, color = Asset)) +
  geom_line(linewidth = 0.8) +
  geom_vline(
    data = threshold_df,
    aes(xintercept = x_int),
    linetype = "dotted",
    color = "black",
    linewidth = 0.6
  ) +
  geom_text(
    data = threshold_df,
    aes(x = x_int, y = 0.95, label = name),
    inherit.aes = FALSE,
    angle = 0,
    vjust = -0.5,
    size = 3.5,
    fontface = "bold"
  ) +
  scale_color_manual(values = c("R1" = "red", "R2" = "blue")) +
  labs(
    title = "Cumulative Distribution Functions of Returns",
    x = NULL,
    y = "Probability (CDF)"
  ) +
  base_theme

p2_cdf <- ggplot(cdf_df, aes(x = x, y = diff)) +
  geom_line(color = "darkgreen", linewidth = 0.8) +
  geom_vline(
    data = threshold_df,
    aes(xintercept = x_int),
    linetype = "dotted",
    color = "black",
    linewidth = 0.6
  ) +
  geom_hline(
    yintercept = 0,
    linetype = "dashed",
    color = "black",
    alpha = 0.5
  ) +
  labs(
    title = "Difference between CDFs (Asset 2 - Asset 1)",
    x = "Return",
    y = "Difference"
  ) +
  base_theme

(p1_cdf / p2_cdf) + plot_layout(heights = c(1.2, 1))
```

```{=latex}
\newpage
```

<!-- # ================================================================== -->
<!-- # Q3(g)  comment on the risk involved in asset 2, compared to asset 1
<!-- # ================================================================== -->

## Comment on the risk involved in asset 2, compared to asset 1. _[4 marks]_ {#qthreePtG}

Based on your answers to parts Q3a–Q3f above, comment on the risk involved in
asset 2, compared to asset 1. _[4 marks]_

```{r Q3-part-g, echo=FALSE}
shortfall_transposed <- data.frame(
  Asset = c("R1", "R2"),
  T1 = c(results_r1[1], results_r2[1]),
  T2 = c(results_r1[2], results_r2[2])
)

shortfall_transposed %>%
  mutate(
    across(starts_with("T"), ~paste0(round(.x * 100, 3)))
  ) %>%
  kable(
    booktabs = TRUE,
    caption = "Shortfall probabilities...*! (values given as percentages)",
    align = "lcc",
    col.names = c("Asset", "Threshold $T_1 = 0$", "Threshold $T_2 = -0.1$"),
    escape = FALSE
  ) %>%
  kable_styling(full_width = FALSE, position = "center")
```

BE note: quick test - refine later~

```{r Q3-testing, echo=FALSE}
thresholds <- c(
  T1 = 0,
  T2 = -0.05,
  T3 = -0.1,
  T4 = -0.15,
  T5 = -0.2,
  T6 = -0.25,
  T7 = -0.5,
  T8 = -0.75
)

results_r1 <- pnorm(thresholds, mean = r1_mu, sd = r1_sd)
results_r2 <- sapply(thresholds, calc_shortfall_r2)

shortfall_transposed <- data.frame(
  Asset = c("R1", "R2"),
  rbind(results_r1, results_r2)
)
# shortfall_transposed %>%
#   mutate(
#     across(where(is.numeric), ~ sprintf("%.3f", .x * 100))
#   ) %>%
#   kable(
#     booktabs = TRUE,
#     caption = "Shortfall probabilities at various thresholds (values given as percentages)",
#     align = "lccccc",
#     col.names = c("Asset", paste0("$", names(thresholds), " = ", thresholds, "$")),
#     escape = FALSE
#   ) %>%
#   kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
#   column_spec(1, bold = TRUE) %>%
#   row_spec(0, bold = TRUE)

shortfall_long <- data.frame(
  Threshold = names(thresholds),
  Value = thresholds,
  R1 = results_r1,
  R2 = results_r2
)

shortfall_long %>%
  mutate(across(
    c(R1, R2),
    ~ formatC(.x * 100, format = "g", digits = 3, flag = "#")
  )) %>%
  kable(
    booktabs = TRUE,
    caption = "Shortfall probabilities - note how probability of getting a loss is greater for asset 2, but the probability of getting a huge loss (after ~5p.c. loss) is more likely for asset 1)",
    col.names = c("Label", "Threshold Value (raw)", "R1 (percent)", "R2 (percent)"),
    align = "llcc"
  ) %>%
  kable_styling(full_width = FALSE, latex_options = "hold_position")
```

BE note: have made above table. Not sure about point below regarding asset 2 having a higher risk of extreme outcomes (it's more likely to lose money generally, but less likely for extreme events, no?)

- Answer might want us to link this to investor types (Asset 1 linked to not liking any type of risk vs R2 being more comfortable with some minor volatility risk, but wanting lower risk of getting completely wiped out)
- Probably also want to mention CDFs crossing - so no one asset clearly less or more risky than the other
- Do we want to mention cross-over point? Think it's ~ -0.05777418
- So this means for any target return higher than about -5.8%, Asset 1 is safer and for any target return lower than -5.8%, Asset 2 is safer?
- Or in other words: Asset 1 has a risk of crashing (left tail) and asset 2 has
  a 'jackpot chance' (right tail).

```{r Q3-cross-over-point, echo=TRUE}
find_crossing <- function(L) {
    pnorm(L, r1_mu, r1_sd) - (r2_p1*pnorm(L, r2_mu1, r2_sd1) + r2_p2*pnorm(L, r2_mu2, r2_sd2))
}
crossing_point <- uniroot(find_crossing, interval = c(-0.1, 0.05))
crossing_point$root
```

Based on the results from $Q3(a)$ to $Q3(f)$, asset 2 appears to be riskier than
asset1, even though their expected returns are similar. Asset 1 follows a single
normal distribution, so its returns are more stable and predictable. In
contrast, asset 2 is a mixture of two normal distributions, which adds more
uncertainty. The shortfall probabilities also indicate that asset 2 has a higer
of risk of extreme outliers. This can be seen in the PDF, where asset 2 shows a
more irregular shape and heavier tails. The CDF also indicates that asset 2 has
a higer of risk of extreme outcomes.

- pdf : Show all final numerical answers in **bold**. Include all working. Paste
  the plots. Write your answer to part Q3g on no more than 5 lines.
- R: Computations and plotting should be done in R (code chunk with heading
  “Q3”).
- [Total: 25 marks (41.7%)]

```{=latex}
\newpage
```

<!-- # Conclusions

TBI -->

<!-- ```{=html} -->
<!-- # References -->
<!-- <div id="refs"></div> -->
<!-- ``` -->

<!-- ```{=latex} -->
<!-- \bibliography{references.bib} -->
<!-- ``` -->

```{=latex}
\newpage
\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

% --- restore default section numbering in the appendix ---
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}
```

# (APPENDIX) Appendices {-}

# TBI

# Supplemental methodology {#apd-supplementary-methods}

## Selection of stocks

Five american insurance stocks were selected...

- They were selected on the basis of...
- Probably ~2 paragraphs here (& could potentially include graph here rather
  than under question 1? Honestly don't know - let's discuss).

## Additional figures (TBC)

```{r appendix-additional-figures, echo=FALSE}
calc_cdf_diff <- function(x) {
  cdf_r1 <- pnorm(x, mean = r1_mu, sd = r1_sd)
  cdf_r2 <- r2_p1 * pnorm(x, r2_mu1, r2_sd1) +
            r2_p2 * pnorm(x, r2_mu2, r2_sd2)
  return(cdf_r2 - cdf_r1)
}
lower_res <- uniroot(calc_cdf_diff, interval = c(-0.2, 0))
lower_x   <- lower_res$root
upper_res <- uniroot(calc_cdf_diff, interval = c(0.1, 0.4))
upper_x   <- upper_res$root

crossovers <- c(Lower = lower_x, Upper = upper_x)
```

```{r Q3-final-combined-plot, eval = TRUE, echo = FALSE, fig.width=7, fig.height=8.0, fig.cap = "Top: pdf, middle: cdfs showing crossover in shortfall probabilities, bottom: difference between R2 and R1 (R2-R1) -> a positive value indicates R2 is riskier and a negative value indicates R1 is riskier.", fig.pos='!ht'}

x_vals <- seq(-0.6, 0.6, by = 0.001)

pdf_r1 <- dnorm(x_vals, mean = r1_mu, sd = r1_sd)
pdf_r2 <- r2_p1 * dnorm(x_vals, mean = r2_mu1, sd = r2_sd1) +
          r2_p2 * dnorm(x_vals, mean = r2_mu2, sd = r2_sd2)

cdf_r1 <- pnorm(x_vals, mean = r1_mu, sd = r1_sd)
cdf_r2 <- r2_p1 * pnorm(x_vals, mean = r2_mu1, sd = r2_sd1) +
          r2_p2 * pnorm(x_vals, mean = r2_mu2, sd = r2_sd2)

plot_df <- tibble(
  x = x_vals,
  PDF_R1 = pdf_r1, PDF_R2 = pdf_r2,
  CDF_R1 = cdf_r1, CDF_R2 = cdf_r2,
  Diff = cdf_r2 - cdf_r1
)

base_theme <- theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
    panel.grid.minor = element_blank(),
    # panel.grid.major.x = element_blank(),
    legend.position = "none"
  )

p1 <- ggplot(plot_df) +
  geom_line(aes(x, PDF_R1, color = "R1"), linewidth = 0.8) +
  geom_line(aes(x, PDF_R2, color = "R2"), linewidth = 0.8) +
  scale_color_manual(values = c("R1" = "red", "R2" = "blue")) +
  labs(title = "Probability density functions", x = NULL, y = "Density") +
  base_theme + theme(legend.position = "top", legend.title = element_blank())

p2 <- ggplot(plot_df) +
  geom_line(aes(x, CDF_R1), color = "red", linewidth = 0.8) +
  geom_line(aes(x, CDF_R2), color = "blue", linewidth = 0.8) +
  geom_vline(xintercept = crossovers, linetype = "dotted", color = "black") +
  annotate(
    "text",
    x = crossovers,
    y = c(0.85, 0.15),
    label = paste0("x = ", round(crossovers, 4)),
    angle = 90,
    vjust = -0.5,
    size = 3,
    fontface = "bold"
  ) +
  labs(title = "Cumulative distribution functions", x = NULL, y = "Prob") +
  base_theme

p3 <- ggplot(plot_df, aes(x, Diff)) +
  geom_area(fill = "darkgreen", alpha = 0.2) +
  geom_line(color = "darkgreen", linewidth = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = crossovers, linetype = "dotted") +
  labs(title = "CDF difference (R2 - R1)", x = "Return", y = "Difference") +
  base_theme

if (knitr::is_html_output()) {
  # convert to plotly individually
  pp1 <- ggplotly(p1)
  pp2 <- ggplotly(p2)
  pp3 <- ggplotly(p3)

  # combine (doesn't work well otherwise)
  subplot(
    pp1,
    pp2,
    pp3,
    nrows = 3,
    shareX = TRUE,
    titleY = TRUE,
    heights = c(0.33, 0.33, 0.33)
  ) %>%
    layout(
      height = 650,
      title = list(
        text = "Test 3 plot plotly",
        x = 0.5,
        xanchor = "center"
      ),
      legend = list(
        orientation = "h",
        x = 0.5,
        xanchor = "center",
        y = -0.1,
        yanchor = "top"
      ),
      margin = list(t = 70, b = 90, l = 70, r = 20),
      hovermode = "x unified"
    ) %>%
    config(responsive = TRUE)
} else {
  (p1 / p2 / p3) + plot_layout(heights = c(1, 1, 0.8))
}
```

## Examining \@ref(qthreePtG) through the lens of utility

Idea: While the probability density functions and cumulative distribution
functions visualised above describe the statistical properties of the
returns, they do not prescribe a decision. To choose between Asset 1
and Asset 2, we must map these returns to an investor's satisfaction
using utility functions.

We evaluate the decision using the three standard models covered in SMM068: Quadratic, Logarithmic, and Power.

As a brief recap, a higher utility score is better as our goal is always to
maximise expected utility.

### Quadratic utility

Quadratic utility assumes the investor cares only about the expected return and the variance. It penalizes volatility regardless of whether that volatility comes from upside or downside moves. The function is defined as:

$$
U(R) = R - \frac{k}{2}R^{2}
$$
Where $k$ represents risk aversion.

<!-- prettier-ignore -->
\begin{align*} 
U(R) &= R - \frac{k}{2}R^{2} \\
\mathbb{E}\big[U(R)\big] &= \mathbb{E}\left[ R - \frac{k}{2}R^{2} \right] \\
&= \mathbb{E}\left[ R \right] - \frac{k}{2}\mathbb{E}\left[R^{2} \right] \\
&= \mathbb{E}\left[ R \right] - \frac{k}{2}\left(\text{Var}(R) - (\mathbb{E}[R])^2 \right)
\end{align*}

So for our assets, where $\mathbb{E}[R_1] = \mathbb{E}[R_2] = 0.06$ and $\text{Var}(R_1) = \text{Var}(R_2) = 0.0244$, the quadratic utility function will be identical for both assets.

### Logarithmic and power utility (CRRA)

Logarithmic utility ($\gamma = 1$) is the baseline for rational growth (often associated with the Kelly Criterion). It assumes wealth cannot drop to zero, and risk aversion adjusts naturally with wealth:

$$
U(R)=\ln(1+R)
$$

A power utility function allows one to fine-tune the investor's risk personality using the coefficient $\gamma$ (gamma):

$$
U(R) = \frac{(1+R)^{1-\gamma}}{1-\gamma}
$$

Where $\gamma>1$ represents one being risk averse and $\gamma<1$ represents one being risk seeking. We can observe how increasing $\gamma$ (representing higher risk aversion) affects the preference for Asset 2 due to its positive skewness.

### Comparative analysis

The table below consolidates the decision-making process using exact integral calculations (eliminating simulation noise) across our key utility frameworks.

```{r exact-utility-table, echo=FALSE, warning=FALSE, message=FALSE}
# BE note: move up if we keep this section
library(tidyverse)

# 1. PDF for Asset 2 (Needed for integration)
pdf_r2 <- function(x) {
  r2_p1 * dnorm(x, r2_mu1, r2_sd1) + r2_p2 * dnorm(x, r2_mu2, r2_sd2)
}

# 2. Dynamic Utility Function Generator
get_crra_func <- function(gamma) {
  if (abs(gamma - 1) < 1e-6) {
    return(function(x) log(1 + x))
  } else {
    return(function(x) ((1 + x)^(1 - gamma)) / (1 - gamma))
  }
}

# 3. Exact Integration Utility Calculator
calc_exact_utility <- function(asset_type, gamma) {
  utility_func <- get_crra_func(gamma)
  if (asset_type == "R1") {
    integrand <- function(x) { utility_func(x) * dnorm(x, r1_mu, r1_sd) }
  } else {
    integrand <- function(x) { utility_func(x) * pdf_r2(x) }
  }
  # Integrate with safe limits to avoid singularity at -1
  return(integrate(integrand, lower = -0.99, upper = 5)$value)
}

# --- Calculate Scores ---

# 1. Quadratic (Algebraic Exact, assuming k = 2)
k_quad <- 2
r1_sec_mom <- r1_sd^2 + r1_mu^2
r2_sec_mom <- (r2_p1 * (r2_sd1^2 + r2_mu1^2)) + (r2_p2 * (r2_sd2^2 + r2_mu2^2))

score_r1_quad <- r1_mu - (k_quad / 2) * r1_sec_mom
score_r2_quad <- (r2_p1 * r2_mu1 + r2_p2 * r2_mu2) - (k_quad / 2) * r2_sec_mom

# 2. CRRA Scores (Gamma 1, 3, 5, 10)
gammas <- c(1, 3, 5, 10)
crra_scores <- map_dfr(gammas, function(g) {
  tibble(
    Model = "Power (CRRA)",
    Assumption = ifelse(g == 1, "Gamma = 1 (Logarithmic)", paste0("Gamma = ", g)),
    R1 = calc_exact_utility("R1", g),
    R2 = calc_exact_utility("R2", g)
  )
})

# Combine and Build Table
results <- bind_rows(
  tibble(Model = "Quadratic", Assumption = "k = 2", R1 = score_r1_quad, R2 = score_r2_quad),
  crra_scores
) %>%
  mutate(
    Diff = R2 - R1,
    Decision = case_when(
      abs(Diff) < 1e-9 ~ "Indifferent",
      Diff > 0 ~ "Choose Asset 2 (Skewed)",
      TRUE ~ "Choose Asset 1 (Normal)"
    )
  )

results %>%
  select(Model, Assumption, R1, R2, Decision) %>%
  kable(
    digits = 5,
    caption = "Comparison of exact expected utility scores (no simulation noise)",
    booktabs = TRUE
  ) %>%
  kable_styling(
    full_width = FALSE,
    position = "center",
    latex_options = "hold_position"
  ) %>%
  column_spec(5, bold = TRUE)
```

### Visualising the decision boundary (power utility function)

By calculating the difference in expected utility $(\mathbb{E}\big[U(R_2)\big] - \mathbb{E}\big[U(R_1)\big])$ continuously across a spectrum of risk aversion, we can see exactly how the preference for positive skewness compounds as $\gamma$ increases.
(!ref - link paper here by Iqbal or collaborators in which this approach was
used).

```{r skewness-preference-plot, echo=FALSE, warning=FALSE, message=FALSE}
# 1. PDF for Asset 2 (Mixture)
pdf_r2 <- function(x) {
  r2_p1 * dnorm(x, r2_mu1, r2_sd1) + r2_p2 * dnorm(x, r2_mu2, r2_sd2)
}

# 2. Dynamic Utility Function Generator
get_crra_func <- function(gamma) {
  if (abs(gamma - 1) < 1e-6) {
    return(function(x) log(1 + x))
  } else {
    return(function(x) ((1 + x)^(1 - gamma)) / (1 - gamma))
  }
}

# 3. Exact Integration Utility Calculator
calc_exact_utility <- function(asset_type, gamma) {
  utility_func <- get_crra_func(gamma)
  if (asset_type == "R1") {
    integrand <- function(x) { utility_func(x) * dnorm(x, r1_mu, r1_sd) }
  } else {
    integrand <- function(x) { utility_func(x) * pdf_r2(x) }
  }
  
  # Set lower = -0.65. This covers >4 standard deviations of our assets 
  # but keeps (1+x) large enough to prevent numerical explosion at gamma = 10.
  return(integrate(integrand, lower = -0.65, upper = 2, subdivisions = 2000)$value)
}

# --- Visualising the Decision Boundary ---

# Gamma Sequence (0 to 10)
gamma_seq <- seq(0, 10, by = 0.1)

# Calculate Difference for each Gamma
plot_data <- map_dfr(gamma_seq, function(g) {
  eu1 <- calc_exact_utility("R1", g)
  eu2 <- calc_exact_utility("R2", g)
  tibble(Gamma = g, Diff = eu2 - eu1)
})

# Plot
ggplot(plot_data, aes(x = Gamma, y = Diff)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  geom_line(color = "darkblue", linewidth = 1.2) +
  geom_ribbon(aes(ymin = 0, ymax = pmax(Diff, 0)), fill = "blue", alpha = 0.1) +
  geom_ribbon(aes(ymin = pmin(Diff, 0), ymax = 0), fill = "red", alpha = 0.1) +
  annotate(
    "text", x = 5, y = max(plot_data$Diff) * 0.75, 
    label = "Prefer Asset 2 (Positive Skew)", color = "blue", fontface = "italic"
  ) +
  labs(
    title = "The skewness preference",
    subtitle = "Since mean and variance are equal, the curve shows the value of skewness",
    x = "Risk aversion coefficient (gamma)",
    y = "Utility advantage (EU_R2 - EU_R1)"
  ) +
  theme_bw()
```


# Reproducibility, accessibility & declarations (Gen-AI & word count?)

<!-- BE note: taken out as we are submitting R file separately -->
<!-- Use: `knitr::purl("cs1-group07.rmd", documentation = 0)` to generate r file -->
<!-- Use: `tools::showNonASCIIfile("cs1-group07.rmd")` to debug -->
<!-- ## R Code Documentation -->
<!-- ```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
``` -->

## Reproducibility & accessibility

An accessible HTML version of this report is available via a public GitHub page:

- https://ytterbiu.github.io/smm068-AS-assignment01-g08/

This report was created in R Markdown. The source code is open source and is
available via:

- https://github.com/ytterbiu/smm068-AS-assignment01-g08

Changes made to this document were tracked using Git and are also available via
the same repository
(https://github.com/ytterbiu/smm068-AS-assignment01-g08/commits/main/). Please
note that this commit history reflects updates made to the project repository
and is not a comprehensive timeline of all contributions by group members.

## Generative AI Declaration

Do we want to include this? - update: yes, asked Iqbal about this on 10/02/2026.

Include this section.

<!--
Generative AI (GenAI) tools were used throughout this project to assist with
project planning, methodological validation, and debugging. In practice, these
tools functioned as an additional technical team member, providing immediate
feedback, troubleshooting code, and challenging statistical assumptions.

### Tools Used

Google Gemini 2.5 Pro (Deep Research), ChatGPT 5

### Prompts used

The GenAI tools were provided with the 'Coursework Group 7.pdf' and extracts
from our R code.

A non-exhaustive list of prompts used is shown below. We opted to include at
least one example of each _type_ of prompt used, rather than a list of all
prompts. For example, if we entered the prompts 'How can we put two ggplots side
by side?' and 'How to remove grey background on a ggplot', we would include only
the first of these in the list below, as both relate to plotting issues (& are
similar to 'querying' with a traditional search engine).

- "Can you help with outlining the steps for this project."
- "...the top of both graphs in r are slightly different. (Pasting R code.) Can
  this be fixed for the plot area?"
- "Help phrasing this in a simple and accessible way - remove any jargon &
  identify the key points"
- "How can this error be tracked down in an rmd file? ! Text line contains an
  invalid character. l.1 ^^@^^@..."
- "Are there specific assumptions for the Kruskal-Wallis test regarding variance
  and where is this explanation in Nonparametric Statistical Methods?"

### Use of GenAI outputs & subsequent revisions

The GenAI tools served as a "consultant" member of the group. Notably, the AI
output:

- Helped debug plotting issues.
- Highlighted potential statistical pitfalls, such as the "ANOVA trap"
  (normality violations) and the importance of ordering data cleansing steps
  correctly.
- Suggested appropriate non-parametric alternatives (Kruskal-Wallis,
  Mann-Whitney U) when diagnostic plots revealed that parametric assumptions
  were violated.
- Recommended specific R packages (nortest, e1071) and functions (ad.test,
  kruskal.test) best suited for each task.
- Commented on the structure of R code snippets and the report structure.

### Changes made:

All AI suggestions were critically reviewed and debated by the group before
implementation. For example:

- While the AI suggested code for complex layouts, the specific aesthetic
  choices and annotations were manually adjusted to ensure clarity.
- Where the AI proposed specific interpretations, these were cross-referenced
  with academic literature and statistics textbooks.
- The final R code for implementation was written and debugged by the group with
  AI snippets being treated as drafts rather than final solutions.

While GenAI was treated as a collaborator, the final analysis, statistical
interpretation, and the written content of this report were generated and
verified by the (human) group members. -->
